<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ANS</title>
    <link href="/2024/09/21/ans/"/>
    <url>/2024/09/21/ans/</url>
    
    <content type="html"><![CDATA[<center><font size=5>Asymmetric numeral systems: entropy coding combining speed of Human coding with compression rate of arithmetic coding</font></center><p>参考博客：<a href="https://bjlkeng.io/posts/lossless-compression-with-asymmetric-numeral-systems/">Lossless Compression with Asymmetric Numeral Systems | Bounded Rationality</a></p><h3 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h3><p>对于数据编码定义以下概念：</p><ul><li><strong>bits</strong>：即数字通信中的0、1。</li><li><strong>symbol</strong>：信息传输的单位，可以为bits或者由一系列bits表示的更高层次的概念，如”a”、”b”等。</li><li><strong>alphabet</strong>：包含传输所有符号的集合。</li><li><strong>message</strong>：由一系列symbols组成。</li></ul><p>数据压缩存在理论上限值，根据香农的信源<a href="https://baike.baidu.com/item/%E4%BF%A1%E6%BA%90%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86/16911659">信源编码定理</a>，数据无损压缩的理论上限即数据本身的熵，即编码每个符号的平均比特数不可能小于：<br>$$<br>H(X)&#x3D;-\sum_{i&#x3D;1}^n p_i\log_2 p_i<br>$$<br>考虑符号${a,b,c}$的两种分布情况$(\frac{4}{7},\frac{2}{7},\frac{1}{7})$与$(\frac{1}{3},\frac{1}{3},\frac{1}{3})$，前者的熵为$H(X)&#x3D;1.3788\text{bits}$，后者为$H(X)&#x3D;1.5850\text{bits}$。这说明平均分布的符号是最难压缩的，因为无法找到符号分布中的不对称性。</p><h3 id="Asymmetric-numeral-systems的介绍"><a href="#Asymmetric-numeral-systems的介绍" class="headerlink" title="Asymmetric numeral systems的介绍"></a>Asymmetric numeral systems的介绍</h3><p>可以假设字节流$b_1b_2b_3\dots b_i$通过二进制转十进制编码为自然数$x_i$，则获得字节$b_{i+1}$时，可以使用以下方式进行编码：<br>$$<br>C(x_i,b_{i+1}):&#x3D;2x_i+b_{i+1}<br>$$<br>同样，基于自然数$x_i$，也可以通过迭代的方式逐步输出$b_i$与$x_{i-1}$：<br>$$<br>(x_{i-1}, b_i)&#x3D;D(x_{i}):&#x3D;(\lfloor \frac{x_i}{2}\rfloor), x_{i} \text{mod}{2})<br>$$<br>有以下注意点：</p><ul><li>为了区分”0”、”00”等，应该初始化$x_0&#x3D;1$。</li><li>上述情况在0、1均匀分布时达到最优，即与二进制转十进制的方式相对应。</li></ul><p>对于字节流也给出奇偶的定义，即以0结尾的字节流视为偶数，以0结尾的字节流视为奇数。</p><p>在上述编码中，$p_0&#x3D;p_1&#x3D;\frac{1}{2}$，字节流的奇数与偶数与自然数$x_i$的奇数与偶数一一对应。对于0、1分布不均匀的情况，假定$p_1&#x3D;p&lt;1-p&#x3D;p_0$，即对于任意一串字节流，以0结尾的概率大于以1结尾的概率，则应期望在给定范围内偶数字节流映射到的自然数的个数大约为奇数字节流映射自然数个数的$\frac{1-p}{p}$。或者说，给定在$[1,N]$范围内的自然数，其中代表奇数字节流与偶数字节流的自然数分别有$N\cdot p$与$N\cdot (1-p)$个。</p><p>此时需要寻找一个最优的编码方式。从信息论的角度分析，自然数$x_i$包含的信息量为$\log_2x_i$，对于出现概率为$p_{b_{i+1}}$的$b_{i+1}$其信息量为$\log_2 \frac{1}{p_{b_{i+1}}}$，则添加该符号后整体的信息量$\log_2 x_{i+1}&#x3D;\log_2 {x_i}+\log_2 \frac{1}{p_{b_{i+1}}}&#x3D;\log_2 \frac{x_i}{p_{b_{i+1}}}$，那么最优的编码方式即为$C_{opt}(x_i,b_{i+1}) \approx \frac{x_i}{p_{b_{i+1}}}$</p><p><img src="https://bjlkeng.io/images/ans_even_odd.png"></p><p>通过上面的分析，对于不均匀分布的0、1找到了最优的编码方式。这也是ABS的思路。</p><h3 id="Uniform-Binary-Variant-uABS"><a href="#Uniform-Binary-Variant-uABS" class="headerlink" title="Uniform Binary Variant(uABS)"></a>Uniform Binary Variant(uABS)</h3><p>继续上面的分析，当0、1满足$p_1&#x3D;p&lt;1-p&#x3D;p_0$时，则前$N$个自然数大约有$N \cdot p$个被映射到1所对应的字符串。为了保证结果为整数，向上取整，则有<br>$$<br>\lceil(N+1)\cdot p\rceil-\lceil N\cdot p\rceil&#x3D;\left\lbrace<br>\begin{aligned}<br>&amp;1,\text{if } b_{b+1}&#x3D;0\newline<br>&amp;0,\text{otherwise}<br>\end{aligned}\right.<br>$$<br>可以证明下面的编码函数满足上式所具有的特性：<br>$$<br>C(x_i,b_{i+1})&#x3D;\left\lbrace<br>\begin{aligned}<br>&amp;\lceil\frac{x_i+1}{1-p} \rceil-1 \quad &amp;\text{if} \space b_{i+1}&#x3D;0\newline<br>&amp;\lfloor \frac{x_i}{p} \rfloor, &amp;\text{otherwise}<br>\end{aligned}<br>\right.<br>$$<br>对应的解码函数$(x_i,b_{i+1})&#x3D;D(x_{i+1})$满足：<br>$$<br>b_{i+1}&#x3D;\lceil(x_{i+1}+1) \cdot p\rceil-\lceil x_{i+1}\cdot p\rceil\<br>x_i&#x3D;\left\lbrace<br>\begin{aligned}<br>&amp;x_{i+1}-\lceil x_{i+1}\cdot p\rceil, &amp;\text{if }b_{i+1}&#x3D;0\newline<br>&amp;\lceil x_{i+1}\cdot p\rceil,&amp; \text{otherwise}<br>\end{aligned}<br>\right.<br>$$<br>uABS的编码过程可以使用映射表可视化：</p><p><img src="https://bjlkeng.io/images/ans_ex3.png"></p><h3 id="Range-Variants-rANS"><a href="#Range-Variants-rANS" class="headerlink" title="Range Variants(rANS)"></a>Range Variants(rANS)</h3><p>uABS还是基于包含0、1的alphabet，实际上可以进一步分析任意尺寸的alphabet。ABS中的编码函数对于符号也是通用的：$C_{opt}(x_i,s_{i+1}) \approx \frac{x_i}{p_{s_{i+1}}}$，即增加一个symbol应只增加$\log_2\frac{x_i}{p_{s_{i+1}}}$比特的信息熵。为了避免大型alphabet对于寻找合适编解码函数的麻烦，将分布量化为$2^n$个块，则$p_s\approx \frac{f_s}{2^n}$，此处$f_s$为自然数。</p><p><img src="https://bjlkeng.io/images/ans_rans.png"></p><p>此时采用以下编解码方式：<br>$$<br>\begin{aligned}<br>&amp;C(x_i, s_{i+1}):&#x3D;\lfloor \frac{x_i}{f_s}\rfloor\cdot 2^n+\text{CDF}[s]+(x_i\text{ mod }f_s)\newline&amp;<br>s_{i+1}&#x3D;\text{symbol}(x_{i+1}\text{ mod }2^n) \text{ such that CDF}[s]\leq x_{i+1}\text{ mod }2^n &lt; \text{CDF}[s+1]\newline&amp;<br>x_i&#x3D;f_s\cdot \lfloor \frac{x_{i+1}}{2^n}\rfloor-\text{CDF}[s]+(x_{i+1}\text{ mod }2^n)&amp;<br>\end{aligned}<br>$$<br>此处$\text{CDF}[s]:&#x3D;\sum_{i&#x3D;1}^{s-1}f_i$。实际上，由于将分布量化为了2的指数幂，乘除可以直接进行移位操作。</p><h3 id="Renormalization"><a href="#Renormalization" class="headerlink" title="Renormalization"></a>Renormalization</h3><p>对于较长的字节流，实际上不能用自然数直接去表示，这里使用到的技巧是保证$x_i\in[2^M,2^{2M}-1]$，即当$x_i$过大时，仅保留最低的$M$位置来保证其位于$[2^M,2^{2M}-1]$，这里的$M$可以设为16等。在解码过程中，如果$x_i$过小，则将当前数字移位并读取$M$为作为低$M$位。伪代码如下：</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs llvm">MASK <span class="hljs-operator">=</span> <span class="hljs-number">2</span>**M - <span class="hljs-number">1</span><br>BOUND <span class="hljs-operator">=</span> <span class="hljs-number">2</span>**(<span class="hljs-number">2</span>*M) - <span class="hljs-number">1</span><br><br># Encoding<br>s <span class="hljs-operator">=</span> readSymbol()<br>x_test <span class="hljs-operator">=</span> (<span class="hljs-keyword">x</span> / f[s]) &lt;&lt; n + (<span class="hljs-keyword">x</span> % f[s]) + <span class="hljs-keyword">c</span>[s]<br>if (x_test &gt; BOUND):<br>    write<span class="hljs-number">16</span>bits(<span class="hljs-keyword">x</span> &amp; MASK)<br>    <span class="hljs-keyword">x</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">x</span> &gt;&gt; M<br><span class="hljs-keyword">x</span> <span class="hljs-operator">=</span> (<span class="hljs-keyword">x</span> / f[s]) &lt;&lt; n + (<span class="hljs-keyword">x</span> % f[s]) + <span class="hljs-keyword">c</span>[s]<br><br># Decoding<br>s <span class="hljs-operator">=</span> symbol[<span class="hljs-keyword">x</span> &amp; MASK]<br>writeSymbol(s)<br><span class="hljs-keyword">x</span> <span class="hljs-operator">=</span> f[s] (<span class="hljs-keyword">x</span> &gt;&gt; n) + (<span class="hljs-keyword">x</span> &amp; MASK) - <span class="hljs-keyword">c</span>[s]<br>if (<span class="hljs-keyword">x</span> &lt; <span class="hljs-number">2</span>**M):<br>    <span class="hljs-keyword">x</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">x</span> &lt;&lt; M + read<span class="hljs-number">16</span>bits()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Compression</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Image Compression</title>
    <link href="/2024/08/18/compression/"/>
    <url>/2024/08/18/compression/</url>
    
    <content type="html"><![CDATA[<center><font size=6>CNN-based Lossy Image Compression</font></center><p>Keyword: lossy image compress, prior model, hyper prior model, context model.</p><h1 id="End-to-end-Optimized-Image-Compression"><a href="#End-to-end-Optimized-Image-Compression" class="headerlink" title="End-to-end Optimized Image Compression"></a><strong><a href="https://arxiv.org/pdf/1611.01704">End-to-end Optimized Image Compression</a></strong></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>提出的观点：图像压缩的目的是对于给定的离散数据设计最小熵的编解码器，极度依赖于对于数据概率分布的先验。一般连续的数据必须要量化为有限的离散值，这引入了误差，这样的压缩即为有损压缩。这种情况下则必须权衡两方面：离散表征的熵(rate)与量化引入的误差(distortion)。</p><p>传统压缩方式是使用线性变换将原始数据转换为合适的连续数值表征，再对其单独进行量化，然后使用无损熵编码对这些量化后的离散表征进行压缩。这种压缩方式又称为变换编码(transform coding)，如JPEG在每个8*8的像素块熵使用离散余弦变换；JPEG2000使用多尺度正交小波分解。一般这里面的三个部分——变换、量化、熵编码是通过人为定义分别优化的。</p><p>端到端的图像压缩模型内，使用级联的线性卷积层和非线性函数作为变换，然后使用均匀量化，之后再使用参数表征的非线性逆变换获得量化后的原始图像。</p><h3 id="Overall-Architecture"><a href="#Overall-Architecture" class="headerlink" title="Overall Architecture"></a>Overall Architecture</h3><img src="/2024/08/18/compression/codingframework.png" class=""><p>提出了非线性编码的总体框架，可实现码率和失真率直接的平衡控制。框架包含：</p><ul><li>$g_a$、$g_s$：实现图像数据域与编码域之间的变换，$y&#x3D;g_a(x;\phi)，\hat x&#x3D;g_s(\hat y;\theta)$，对$y$进行量化得到$q$，对$q$进行编码得到对应的码率$R$</li><li>$g_p$：一个固定的变换，实现将图像转换到感知域$\hat z&#x3D;g_p(\hat x)$，进行原图像与重建图像的对比，得到失真度$D$</li></ul><p>对上述模型的优化集中在$\phi$与$\theta$上，可定义损失函数为$R+\lambda D$，通过修改$\lambda$来控制模型的R-D性能。</p><h3 id="Choice-of-Forward-Inverse-and-Perceptual-Transforms"><a href="#Choice-of-Forward-Inverse-and-Perceptual-Transforms" class="headerlink" title="Choice of Forward, Inverse, and Perceptual Transforms"></a>Choice of Forward, Inverse, and Perceptual Transforms</h3><p>$g_a$包含卷积、下采样与GDN。以$u_i^{(k)}(m,n)$代表第$k$层第$i$个通道，位置为$(m,n)$的元素。</p><p>则输入图像为$u_i^{(0)}(m,n)$，输出为$u_i^{(3)}(m,n)$，每一阶段可表示为先卷积<br>$$<br>v_i^{(k)}(m,n)&#x3D;\sum_j (h_{k,ij} * u_j^{(k)})(m,n)+c_{k,i}<br>$$<br>再下采样<br>$$<br>w_i^{(k)}(m,n)&#x3D;v_i^{(k)}(s_km,s_kn)<br>$$<br>之后进行GDN操作<br>$$<br>u_i^{(k+1)}(m,n)&#x3D;\frac{w_i^{(k)}(m,n)}{\left(\beta_{k,i}+\sum_j\gamma_{k.ij}(w_j^{(k)}(m,n))^2\right)^{\frac12}}<br>$$<br>对于解码方式，则与上述操作相反，分别用反卷积、上采样、IGDN代替。</p><h3 id="Optimization-of-Nonlinear-Transform-Coding-Model"><a href="#Optimization-of-Nonlinear-Transform-Coding-Model" class="headerlink" title="Optimization of Nonlinear Transform Coding Model"></a>Optimization of Nonlinear Transform Coding Model</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>$$<br>L[g_a,g_s,P_q]&#x3D;-\mathbb E[\log_2 P_q]+\lambda\mathbb E[d(z,\hat z)]<br>$$</p><p>这里的期望由数据集所有图片求均值得到。为了编码使用更小的码率，要求编码的熵尽可能小，即降低$-\mathbb E[\log_2 P_q]$，为了保证图片重建质量，则需与原图的差距尽可能小，即降低$\mathbb E[d(z,\hat z)]$</p><h4 id="可微分量化"><a href="#可微分量化" class="headerlink" title="可微分量化"></a>可微分量化</h4><p>一般的分段量化遵循：<br>$$<br>\hat y_i&#x3D;q_i&#x3D;\text{round}(y_i)<br>$$<br>此时$\hat y_i$的分布可表示为脉冲函数的加权积分：<br>$$<br>P_{q_i}(n)&#x3D;\int_{n-\frac12}^{n+\frac12}p_{y_i}(t)dt, \text{ for all }n\in \mathbb Z<br>$$<br>上述的分段量化会存在边界不可导、其余位置导数为0的问题。论文中使用加性均匀噪声，即在$y$上添加了服从$\mathcal U(-0.5,0.5)$的噪声$\Delta y$：$\tilde{y}&#x3D;y+\Delta y$来替代对$y$直接进行分段量化$q$，则<br>$$<br>p_{\tilde y}(n)&#x3D;P_q(n) \text{ for all }n\in\mathbb Z^M<br>$$<br>此时$\tilde y$的可微分熵可用于近似$q$的熵。</p><p>这里$p_{\tilde y_i}&#x3D;p_{y_i}*\mathcal U(0,1)$相当于对$p_{y_i}$进行平滑，模型的量化误差可以通过减小均匀分布的范围进行任意的缩小。</p><p>使用上述加性均匀噪声的量化方法，损失函数可写为<br>$$<br>L(\theta, \phi)&#x3D;\mathbb E_{x,\Delta y}\left[-\sum_i \log_2p_{\tilde y_i}(g_a(x;\phi)+\Delta y;\psi^{(i)})+\lambda d\left(g_p(g_s(g_a(x;\phi)+\Delta y;\theta)),g_p(x)\right)\right]<br>$$</p><h4 id="与VAE的关联"><a href="#与VAE的关联" class="headerlink" title="与VAE的关联"></a>与VAE的关联</h4><p>在VAE的推理过程中，需要模型找到后验分布$p_{y|x}(y|x)$。可以通过定义分布$q(y|x)$计算KL散度去进行拟合：<br>$$<br>D_{KL}[q||p_{x|y}]&#x3D;\mathbb E_{y\sim q}\log q(y|x)-\mathbb E_{y\sim q}\log P_{y|x}(y|x)\<br>&#x3D;\mathbb E_{y\sim q}\log q(y|x)-\mathbb E_{y\sim q}\log p_{x|y}(x|y)-\mathbb E_{y\sim q}\log p_y(y)+\text{const}<br>$$</p><p>定义生成模型（解码器）为<br>$$<br>p_{x|\tilde y}(x|\tilde{y};\lambda,\theta)&#x3D;\mathcal N (x;g_s(\tilde y;\theta),(2\lambda)^{-1} \mathbf{1})\newline<br>p_{\tilde y}(\tilde y;\psi^{(0)},\psi^{(1)},…)&#x3D;\prod_i p_{\tilde y_i}(\tilde y_i;\psi^{(i)})<br>$$<br>近似的后验分布为<br>$$<br>q(\tilde y|x;\phi)&#x3D;\prod_i \mathcal U(\tilde y_i;y_i,1) \text{ with }y&#x3D;g_a(x;\phi)<br>$$<br>此处$\mathcal U(\tilde y_i;y_i,1)$代表中心位于$y_i$，单位长度的均匀分布。</p><p>此时，KL散度第一项为常数，第二项对应失真度，第三项对应码率。</p><p>VAE的图像重建任务在对应于$R+\lambda D$中$\lambda\rightarrow \infty$的情况。</p><h1 id="Variational-Image-Compression-With-a-Scale-Hyperprior"><a href="#Variational-Image-Compression-With-a-Scale-Hyperprior" class="headerlink" title="Variational Image Compression With a Scale Hyperprior"></a><a href="https://arxiv.org/pdf/1802.01436"><strong>Variational Image Compression With a Scale Hyperprior</strong></a></h1><p>传统压缩方法可通过传送<em>边信息</em>(side information, 从编码器发送给解码器的多余比特信息，用于调整熵编码模型来减少错配)。以这种方式，要求平均的边信息比特量要少于减少到编码比特量。</p><p>比如，JPEG对每个图像均使用$8\times8$的分块方式，对于大面积的低频区域，实际上可以使用更大的分块来有效进行编码，而HEVC则可以预先选择分块的大小，并将其存储在边信息中。</p><p>论文中基于该问题，提出了对熵编码模型的隐含表示的学习。论文中尝试证明边信息可以视为熵编码模型的先验条件，即作为hyperprior。</p><p>码率对应压缩表征的期望码长，可以使用交叉熵进行表示：<br>$$<br>R&#x3D;\mathbb E_{\hat{y}\sim m}[-\log_2p_{\hat y}(\hat y)]&#x3D;\mathbb E_{x\sim{p_x}}[-\log_2p_{\hat{y}}(Q(g_a(x;\phi_g)))]<br>$$</p><h3 id="Introduction-of-a-Scale-Hyperprior"><a href="#Introduction-of-a-Scale-Hyperprior" class="headerlink" title="Introduction of a Scale Hyperprior"></a>Introduction of a Scale Hyperprior</h3><p>对于$\hat y$，在空间域上不同元素直接存在相关性，因此引入了额外的随机变量$\hat z$来消除$\hat y$在空间上的相关性。</p><p>每个元素$\hat y_i$被建模为零均值高斯分布，方差对应为$\sigma_i$，其值由$\hat z$通过变换$h_s$预测得到：<br>$$<br>p_{\tilde y|\tilde z}(\tilde y|\tilde z,\theta_h)&#x3D;\prod_i \left(\mathcal N(0,\tilde \sigma_i^2)*\mathcal U(-\frac12,\frac12)\right)(\tilde y_i)\text{ with }\tilde \sigma&#x3D;h_s(\tilde z;\theta_h)<br>$$<br>实现的模型为在原先模型上添加从$y$至$z$的变换$h_a$，从而构建可分解的联合后验分布：<br>$$<br>q(\tilde y,\tilde z|x,\phi_g,\phi_h)&#x3D;\prod_i\mathcal U(\tilde y_i|y_i-\frac12,y_i+\frac12)\cdot \prod_j\mathcal U(\tilde z_j|z_j-\frac12,z_j+\frac12)\newline<br>\text{ with }y&#x3D;g_a(x;\phi_g),z&#x3D;h_a(y;\phi_h)<br>$$<br>对$\tilde z$进行建模：<br>$$<br>p_{\tilde z|\psi}(\tilde z|\psi)&#x3D;\prod_i\left(p_{z_i|\psi^{(i)}}(\psi^{(i)}*\mathcal U(-\frac12,\frac12)\right)(\tilde z_i)<br>$$</p><h1 id="Joint-Autoregressive-and-Hierarchical-Priors-for-Learned-Image-Compression"><a href="#Joint-Autoregressive-and-Hierarchical-Priors-for-Learned-Image-Compression" class="headerlink" title="Joint Autoregressive and Hierarchical Priors for Learned Image Compression"></a><a href="https://arxiv.org/pdf/1809.02736">Joint Autoregressive and Hierarchical Priors for Learned Image Compression</a></h1><img src="/2024/08/18/compression/minnen2018.png" class=""><p>在balle2018的基础上将scale hyperprior替换为了mean-scale hyperprior，同时引入了context model。</p><h1 id="Learned-Image-Compression-with-Discretized-Gaussian-Mixture-Likelihoods-and-Attention-Modules"><a href="#Learned-Image-Compression-with-Discretized-Gaussian-Mixture-Likelihoods-and-Attention-Modules" class="headerlink" title="Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules"></a><a href="https://arxiv.org/pdf/2001.01568">Learned Image Compression with Discretized Gaussian Mixture Likelihoods and Attention Modules</a></h1><h1 id="ELIC-Efficient-Learned-Image-Compression-with-Unevenly-Grouped-Space-Channel-Contextual-Adaptive-Coding"><a href="#ELIC-Efficient-Learned-Image-Compression-with-Unevenly-Grouped-Space-Channel-Contextual-Adaptive-Coding" class="headerlink" title="ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding"></a><a href="https://arxiv.org/pdf/2203.10886">ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding</a></h1>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Git</title>
    <link href="/2024/08/18/git/"/>
    <url>/2024/08/18/git/</url>
    
    <content type="html"><![CDATA[<p>Git为分布式版本控制系统。</p><p>Git的本地数据管理分为三个区域，分别为工作区、暂存区和本地仓库。</p><ul><li>工作区：本地的目录</li><li>暂存区：临时存储区域，用于保存即将提交到Git仓库的修改内容。</li><li>本地仓库：包含了完整的项目历史和元数据，是Git存储代码和版本信息的主要位置。</li></ul><p>每次修改文件，都会暂时保存在暂存区</p><p>文件的状态：</p><ul><li>未跟踪(Untrack)：新创建还未被Git管理起来的文件。</li><li>未修改(Unmodified)：已被Git管理起来，但是文件内容没有发生变化。</li><li>已修改(Modified)：已修改过文件，但未被添加到暂存区内。</li><li>已暂存(Staged)：已修改并添加到暂存区的文件。</li></ul><img src="/2024/08/18/git/filestatus.png" class=""><p>Git的使用方法</p><p>初始化仓库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git init<br></code></pre></td></tr></table></figure><p>查看当前仓库状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git status<br></code></pre></td></tr></table></figure><p>将文件添加到暂存区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git add &lt;file&gt;<br></code></pre></td></tr></table></figure><p>支持通配符，比如将所有<code>py</code>格式的文件添加到暂存区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git add *.py<br></code></pre></td></tr></table></figure><p>将暂存区文件提交到本地仓库中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git commit -m <span class="hljs-string">&quot;commit-info&quot;</span><br></code></pre></td></tr></table></figure><p>对于包含子模块的项目，需要使用以下命令更新子模块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git submodule update --init --recursive<br></code></pre></td></tr></table></figure><p>查看提交记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><p>只查看每次提交的ID与信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">log</span> --oneline<br></code></pre></td></tr></table></figure><p><code>git reset</code>可以用于版本回退</p><p>参数：</p><ul><li><p><code>--mixed</code>：保留工作区的文件，不保留暂存区文件。</p></li><li><p><code>--soft</code>：保留工作区和暂存区的文件，可以回到提交本版本之前的状态。</p></li><li><p><code>--hard</code>：工作区文件、暂存区文件均不保留，如果认为本次修改的内容没有太大作用，则可以指定该参数回溯到上个版本。</p></li></ul><p><code>git diff</code>可以用于查看工作区、暂存区、本地仓库之间的差异，查看不同版本之间的差异，以及查看不同分支之间的差异。</p>]]></content>
    
    
    <categories>
      
      <category>Tools</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>ViT</title>
    <link href="/2024/08/14/ViT/"/>
    <url>/2024/08/14/ViT/</url>
    
    <content type="html"><![CDATA[<center><font size=6>Vision Transformer</font></center><h3 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h3><img src="/2024/08/14/ViT/vit.png" class=""><p>ViT首次将Transformer应用到图像处理领域，属于Transformer的Encoder-only类型。相比同参数量的CNN，经过大规模数据集训练后有着更好的效果。</p><h3 id="ViT中使用到的方法"><a href="#ViT中使用到的方法" class="headerlink" title="ViT中使用到的方法"></a>ViT中使用到的方法</h3><h4 id="分块-Patch"><a href="#分块-Patch" class="headerlink" title="分块(Patch)"></a>分块(Patch)</h4><p>将Transformer应用到图像上，首先面临的问题就是token的规模过大，将占用大量内存。ViT中先将图像进行分块展平，再使用线性层映射到低维输入到Transformer中，有效地减少了token的长度。</p><p>对于2D图片$\mathbf x\in \mathbb{R}^{H\times W\times C}$，将其分割成$P\times P$的patch，展平得到$\mathbf x_p\in \mathbb{R}^{N\times (P^2\cdot C)},N&#x3D;\frac{HW}{P^2}$。$N$对应Transformer输入序列的有效长度，而Transformer使用的向量长度为$D$，则需要使用可学习的线性映射将图像patch映射到$D$维上，即$\mathbf x_p\mathbf E\in \mathbb{R}^{N\times D}$。</p><p>将图像转为embedding的方式：</p><ul><li><p>将图像分块，每一块展平作为一个token。</p></li><li><p>使用二维卷积预处理，将得到特征图flatten。</p></li></ul><h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>使用可学习的一维位置编码，让模型自行学习图片像素的位置关系。</p><p>CNN的结构有以下特性：</p><ul><li>相邻区域具有相似特征</li><li>平移不变性</li></ul><p>ViT相比CNN，消除了这些归纳偏置。Encoder中只有MLP具有局部、平移不变的特性，而自注意力层则会关注全局特性。</p><h4 id="Class-Token"><a href="#Class-Token" class="headerlink" title="Class Token"></a>Class Token</h4><p>Class token的机制源于BERT，ViT希望也能将其用于图像分类任务中，提出在输入的token中加入一个class token作为第0个向量，与由图像得到的token一同输入到Transformer中。最后取Transformer输出中的第0个向量作为分类预测的依据。另一种处理方式为对输出的图像token进行平均池化，作为分类判断的特征。</p><p>ViT为了验证Transformer在图像处理中也能有优秀的处理能力，使用了前者。</p><h4 id="Transformer-Encoder"><a href="#Transformer-Encoder" class="headerlink" title="Transformer Encoder"></a>Transformer Encoder</h4><p>ViT的编码器设计与Transformer的编码器结构相同，由多个包含多头自注意力和MLP的块连接而成。每个块中包含两个残差结构，在残差结构的头部使用LayerNorm。MLP包含两层，使用GELU作为激活函数。</p><p>$$<br>\begin{aligned}<br>\mathbf{z_0}&amp;&#x3D;[\mathbf x_{\text{class}};\mathbf x_{p}^{1}\mathbf{E};\mathbf{x}_{p}^2\mathbf{E};\dots \mathbf{x}_{p}^N\mathbf{E}]+\mathbf{E}_{pos}, &amp; \mathbf{E} \in \mathbb{R}^{(P^2 \cdot C)\times D}, \mathbf{E}_{\text{pos}}\in\mathbb{R}^{(N+1)\times D}\newline<br>\mathbf {z}_l’&amp;&#x3D; \text{MSA}(\text{LN}(\mathbf {z}_{l-1}))+\mathbf {z}_{l-1}, &amp; l&#x3D;1\dots L\newline<br>\mathbf{z}_l &amp;&#x3D; \text{MLP}(\text{LN}(\mathbf{z}’_l))+\mathbf{z}’_l, &amp; l&#x3D;1\dots L\newline<br>\mathbf{y} &amp;&#x3D; \text{LN}(\mathbf{z}_L^0)<br>\end{aligned}<br>$$</p><p>多头注意力机制：</p><ul><li><p>标准自注意力机制：</p><p>通过MLP将序列$\mathbf z\in \mathbb R ^{N\times D}$映射到$q,k,v\in\mathbb R^{N\times D_h}$。注意力的权重$A_{ij}$由$q,k$的相似程度决定，并以加权的方式作用到$v$上：<br>$$<br>\begin{aligned}<br>\left[\mathbf q, \mathbf k, \mathbf v\right]&amp;&#x3D;\mathbf z \mathbf U_{qkv}, &amp;\mathbf U_{qkv}\in \mathbb R^{D\times 3D_h} \newline<br>A&amp;&#x3D;\text{softmax}(\mathbf q\mathbf k^\top&#x2F;\sqrt{D_h}), &amp;A\in \mathbb R^{N\times N}\newline<br>\text{SA}(\mathbf z)&amp;&#x3D;A\mathbf v<br>\end{aligned}<br>$$</p></li><li><p>多头自注意力机制：将输入序列$\mathbf z\in \mathbb R ^{N\times D}$拆分为$k$个等长的序列$\mathbf z_i\in\mathbb R^{N\times \frac Dk}$，并行运行$k$个自注意力操作，最后将多头的输出拼接为一个输出。</p></li></ul><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p><a href="https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py">vit-pytorch&#x2F;vit_pytorch&#x2F;vit.py</a></p><img src="/2024/08/14/ViT/encoder.png" class=""><p><code>FeedForward</code>对应Transformer Encoder内的MLP前馈部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">FeedForward</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, hidden_dim, dropout = <span class="hljs-number">0.</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.net = nn.Sequential(<br>            nn.LayerNorm(dim),<br>            nn.Linear(dim, hidden_dim),<br>            nn.GELU(),<br>            nn.Dropout(dropout),<br>            nn.Linear(hidden_dim, dim),<br>            nn.Dropout(dropout)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.net(x)<br></code></pre></td></tr></table></figure><p><code>Attention</code>对应Transformer Encoder内的Attention部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, heads = <span class="hljs-number">8</span>, dim_head = <span class="hljs-number">64</span>, dropout = <span class="hljs-number">0.</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        inner_dim = dim_head *  heads<span class="hljs-comment"># 将输入的维度按照多头注意力机制划分</span><br>        project_out = <span class="hljs-keyword">not</span> (heads == <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> dim_head == dim) <span class="hljs-comment"># 判断是否为多头自注意力</span><br><br>        <span class="hljs-variable language_">self</span>.heads = heads<br>        <span class="hljs-variable language_">self</span>.scale = dim_head ** -<span class="hljs-number">0.5</span><br><br>        <span class="hljs-variable language_">self</span>.norm = nn.LayerNorm(dim)<br><br>        <span class="hljs-variable language_">self</span>.attend = nn.Softmax(dim = -<span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(dropout)<br><br>        <span class="hljs-variable language_">self</span>.to_qkv = nn.Linear(dim, inner_dim * <span class="hljs-number">3</span>, bias = <span class="hljs-literal">False</span>)<br><br>        <span class="hljs-variable language_">self</span>.to_out = nn.Sequential(<br>            nn.Linear(inner_dim, dim),<br>            nn.Dropout(dropout)<br>        ) <span class="hljs-keyword">if</span> project_out <span class="hljs-keyword">else</span> nn.Identity()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">        x: torch.Tensor, token with dimension: [B, N, D]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = <span class="hljs-variable language_">self</span>.norm(x)<br><br>        qkv = <span class="hljs-variable language_">self</span>.to_qkv(x).chunk(<span class="hljs-number">3</span>, dim = -<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># qkv: List[torch.Tensor]</span><br>        q, k, v = <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> t: rearrange(t, <span class="hljs-string">&#x27;b n (h d) -&gt; b h n d&#x27;</span>, h = <span class="hljs-variable language_">self</span>.heads), qkv)<br><span class="hljs-comment"># q, k, v: [N, D, heads, dim_head]</span><br>        dots = torch.matmul(q, k.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)) * <span class="hljs-variable language_">self</span>.scale<br><span class="hljs-comment"># dots: [N, D, heads, heads]</span><br>        attn = <span class="hljs-variable language_">self</span>.attend(dots)<br>        attn = <span class="hljs-variable language_">self</span>.dropout(attn)<br><br>        out = torch.matmul(attn, v)<br>        out = rearrange(out, <span class="hljs-string">&#x27;b h n d -&gt; b n (h d)&#x27;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.to_out(out)<br></code></pre></td></tr></table></figure><p>ViT的Transformer部分（只需要Encoder）：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Transformer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, depth, heads, dim_head, mlp_dim, dropout = <span class="hljs-number">0.</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.norm = nn.LayerNorm(dim)<br>        <span class="hljs-variable language_">self</span>.layers = nn.ModuleList([])<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depth):<br>            <span class="hljs-variable language_">self</span>.layers.append(nn.ModuleList([<br>                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),<br>                FeedForward(dim, mlp_dim, dropout = dropout)<br>            ]))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> attn, ff <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers:<br>            x = attn(x) + x<br>            x = ff(x) + x<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.norm(x)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ViT</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = <span class="hljs-string">&#x27;cls&#x27;</span>, channels = <span class="hljs-number">3</span>, dim_head = <span class="hljs-number">64</span>, dropout = <span class="hljs-number">0.</span>, emb_dropout = <span class="hljs-number">0.</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        image_height, image_width = pair(image_size)<br>        patch_height, patch_width = pair(patch_size)<br><br>        <span class="hljs-keyword">assert</span> image_height % patch_height == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> image_width % patch_width == <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;Image dimensions must be divisible by the patch size.&#x27;</span><br><br>        num_patches = (image_height // patch_height) * (image_width // patch_width)<br>        patch_dim = channels * patch_height * patch_width<br>        <span class="hljs-keyword">assert</span> pool <span class="hljs-keyword">in</span> &#123;<span class="hljs-string">&#x27;cls&#x27;</span>, <span class="hljs-string">&#x27;mean&#x27;</span>&#125;, <span class="hljs-string">&#x27;pool type must be either cls (cls token) or mean (mean pooling)&#x27;</span><br><br>        <span class="hljs-comment"># 将patch的维度映射到Transformer能处理的token的长度</span><br>        <span class="hljs-variable language_">self</span>.to_patch_embedding = nn.Sequential(<br>            Rearrange(<span class="hljs-string">&#x27;b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)&#x27;</span>, p1 = patch_height, p2 = patch_width),<br>            nn.LayerNorm(patch_dim),<br>            nn.Linear(patch_dim, dim),<br>            nn.LayerNorm(dim),<br>        )<br><br>        <span class="hljs-variable language_">self</span>.pos_embedding = nn.Parameter(torch.randn(<span class="hljs-number">1</span>, num_patches + <span class="hljs-number">1</span>, dim))<br>        <span class="hljs-variable language_">self</span>.cls_token = nn.Parameter(torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, dim))<br>        <span class="hljs-variable language_">self</span>.dropout = nn.Dropout(emb_dropout)<br><br>        <span class="hljs-variable language_">self</span>.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)<br><br>        <span class="hljs-variable language_">self</span>.pool = pool<br>        <span class="hljs-variable language_">self</span>.to_latent = nn.Identity()<br><br>        <span class="hljs-variable language_">self</span>.mlp_head = nn.Linear(dim, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, img</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">        img: torch.Tensor, [B, C, H, W]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x = <span class="hljs-variable language_">self</span>.to_patch_embedding(img)<br>        <span class="hljs-comment"># x: [B, N, D]</span><br>        b, n, _ = x.shape<br><br>        cls_tokens = repeat(<span class="hljs-variable language_">self</span>.cls_token, <span class="hljs-string">&#x27;1 1 d -&gt; b 1 d&#x27;</span>, b = b)<br>        x = torch.cat((cls_tokens, x), dim=<span class="hljs-number">1</span>)<br>        x += <span class="hljs-variable language_">self</span>.pos_embedding[:, :(n + <span class="hljs-number">1</span>)]<br>        x = <span class="hljs-variable language_">self</span>.dropout(x)<br><br>        x = <span class="hljs-variable language_">self</span>.transformer(x)<br><br>        x = x.mean(dim = <span class="hljs-number">1</span>) <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.pool == <span class="hljs-string">&#x27;mean&#x27;</span> <span class="hljs-keyword">else</span> x[:, <span class="hljs-number">0</span>]<br><br>        x = <span class="hljs-variable language_">self</span>.to_latent(x)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.mlp_head(x)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch学习</title>
    <link href="/2022/06/15/Pytorch%E5%AD%A6%E4%B9%A0/"/>
    <url>/2022/06/15/Pytorch%E5%AD%A6%E4%B9%A0/</url>
    
    <content type="html"><![CDATA[<h1 id="Python基础语法"><a href="#Python基础语法" class="headerlink" title="Python基础语法"></a>Python基础语法</h1><h2 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h2><h3 id="定义函数"><a href="#定义函数" class="headerlink" title="定义函数"></a>定义函数</h3><ul><li><p>定义函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>():<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hello!&quot;</span>)<br><br><br>func()<br></code></pre></td></tr></table></figure></li><li><p>向函数传递信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>(<span class="hljs-params">username</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Hello, &quot;</span> + username.title() + <span class="hljs-string">&quot;!&quot;</span>)<br>    <br>fun(<span class="hljs-string">&#x27;tom&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="实参与形参"><a href="#实参与形参" class="headerlink" title="实参与形参"></a>实参与形参</h3><ul><li><p>关键字实参与默认值</p><p><strong>使用默认值时，在形参列表中需要先列出没有默认值的形参</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">describe</span>(<span class="hljs-params">name，<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;cat&#x27;</span></span>):<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;My &quot;</span> + <span class="hljs-built_in">type</span>.name + <span class="hljs-string">&quot;&#x27;s name is &#x27;&quot;</span> + name.title() + <span class="hljs-string">&quot;.&quot;</span>)<br>    <br>describe(name=<span class="hljs-string">&#x27;tom&#x27;</span>)<br>describe(<span class="hljs-string">&#x27;tom&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><ul><li><p>返回简单值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_name</span>(<span class="hljs-params">first_name,last_name</span>):<br>    full_name = first_name + <span class="hljs-string">&#x27;&#x27;</span> + last_name<br>    <span class="hljs-keyword">return</span> full_name.title()<br><br>musician = get_formatted_name(<span class="hljs-string">&#x27;jimi&#x27;</span>,<span class="hljs-string">&#x27;hendrix&#x27;</span>)<br><span class="hljs-built_in">print</span>(musician)<br></code></pre></td></tr></table></figure></li><li><p>让实参变为可选的</p><p>若参数可能没有传入，则将其默认值设为空</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_name</span>(<span class="hljs-params">first_name,last_name,middle_name=<span class="hljs-string">&#x27;&#x27;</span></span>):<br>    <span class="hljs-keyword">if</span> middle_name:<br>    full_name = first_name + <span class="hljs-string">&#x27;&#x27;</span> + middle_name + <span class="hljs-string">&#x27;&#x27;</span> + last_name<br>    <span class="hljs-keyword">else</span>:<br>        full_name = first_name + <span class="hljs-string">&#x27;&#x27;</span> + last_name<br>    <span class="hljs-keyword">return</span> full_name.title()<br><br>musician = get_formatted_name(<span class="hljs-string">&#x27;jimi&#x27;</span>,<span class="hljs-string">&#x27;hendrix&#x27;</span>)<br>musician = get_formatted_name(<span class="hljs-string">&#x27;john&#x27;</span>,<span class="hljs-string">&#x27;hooker&#x27;</span>,<span class="hljs-string">&#x27;lee&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>返回字典</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_person</span>(<span class="hljs-params">first_name,last_name</span>):<br>person = &#123;<span class="hljs-string">&#x27;first&#x27;</span>:first_name, <span class="hljs-string">&#x27;last&#x27;</span>:last_name&#125;<br>    <span class="hljs-keyword">return</span> person<br><br>musician = build_person(<span class="hljs-string">&#x27;jimi&#x27;</span>,<span class="hljs-string">&#x27;hendrix&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="传递列表"><a href="#传递列表" class="headerlink" title="传递列表"></a>传递列表</h3><ul><li><p>传递列表</p><p>将列表传递给函数后，函数就能直接访问内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">names</span>):<br>    <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> names:<br>        msg = <span class="hljs-string">&quot;Hello &quot;</span> + name.title() + <span class="hljs-string">&quot;!&quot;</span><br>        <span class="hljs-built_in">print</span>(msg)<br>        <br>usernames = [<span class="hljs-string">&#x27;hannah&#x27;</span>,<span class="hljs-string">&#x27;ty&#x27;</span>]<br>greet(usernames)<br></code></pre></td></tr></table></figure></li><li><p>在函数中修改列表</p><p>将列表传递给函数后，函数对列表所做的修改都是永久性的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">func</span>(<span class="hljs-params">unfinished,completed</span>):<br>    <span class="hljs-keyword">while</span> unfinished:<br>        current = unfinished.pop()<br>        completed.append(current)<br></code></pre></td></tr></table></figure></li><li><p>禁止函数修改列表</p><p>可以通过向函数传递列表副本，从而不改变列表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">func(list_name[:])<br></code></pre></td></tr></table></figure></li></ul><h3 id="传递任意数量的实参"><a href="#传递任意数量的实参" class="headerlink" title="传递任意数量的实参"></a>传递任意数量的实参</h3><ul><li><p>传递任意数量的实参</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_pizza</span>(<span class="hljs-params">*topping</span>):<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nMaking a pizza needs:&quot;</span>)<br>    <span class="hljs-keyword">for</span> topping <span class="hljs-keyword">in</span> toppings:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;- &#x27;</span> + topping)<br>        <br>make_pizza(<span class="hljs-string">&#x27;mushroom&#x27;</span>,<span class="hljs-string">&#x27;green peppers&#x27;</span>,<span class="hljs-string">&#x27;extra cheese&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>结合使用位置实参和任意数量实参</p><p><strong>必须将接纳任意数量实参的形参放在最后</strong>，Python优先匹配位置实参和关键字实参，再将余下的实参收集到最后一个形参中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_pizza</span>(<span class="hljs-params">size,*topping</span>):<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nMaking a pizza needs:&quot;</span>)<br>    <span class="hljs-keyword">for</span> topping <span class="hljs-keyword">in</span> toppings:<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;- &#x27;</span> + topping)<br>        <br>make_pizza(<span class="hljs-number">12</span>,<span class="hljs-string">&#x27;mushroom&#x27;</span>,<span class="hljs-string">&#x27;green peppers&#x27;</span>,<span class="hljs-string">&#x27;extra cheese&#x27;</span>)<br></code></pre></td></tr></table></figure></li><li><p>使用任意数量的关键字实参</p><p>创建字典接收任意数量的键值对。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_profile</span>(<span class="hljs-params">first,last,**user_info</span>):<br>    profile = &#123;&#125;<br>    profile[<span class="hljs-string">&#x27;first_name&#x27;</span>] = first<br>    profile[<span class="hljs-string">&#x27;last_name&#x27;</span>] = last<br>    <span class="hljs-keyword">for</span> key,value <span class="hljs-keyword">in</span> user_info.items():<br>        profile[key] = value<br>    <span class="hljs-keyword">return</span> profile<br><br>user_profile = build_profile(<span class="hljs-string">&#x27;albert&#x27;</span>,<span class="hljs-string">&#x27;einstein&#x27;</span>,location=<span class="hljs-string">&#x27;princeton&#x27;</span>,field=<span class="hljs-string">&#x27;physics&#x27;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="模块、函数导入"><a href="#模块、函数导入" class="headerlink" title="模块、函数导入"></a>模块、函数导入</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入整个模块</span><br><span class="hljs-keyword">import</span> module_name<br>module_name.func_name()<br><br><span class="hljs-comment"># 导入特定函数</span><br><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> func_name<br>func_name()<br><br><span class="hljs-comment"># 使用as给函数指定别名</span><br><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> func_name <span class="hljs-keyword">as</span> fn<br>fn()<br><br><span class="hljs-comment"># 使用as给模块指定别名</span><br><span class="hljs-keyword">import</span> module_name <span class="hljs-keyword">as</span> mn<br>mn.func_name()<br><br><span class="hljs-comment"># 导入模块中所有函数(不建议使用，因为当模块中函数存在重名时，会出现函数覆盖，可能引发错误)</span><br><span class="hljs-keyword">from</span> module_name <span class="hljs-keyword">import</span> *<br>func_name()<br></code></pre></td></tr></table></figure><h2 id="类"><a href="#类" class="headerlink" title="类"></a>类</h2><h3 id="类的创建与使用"><a href="#类的创建与使用" class="headerlink" title="类的创建与使用"></a>类的创建与使用</h3><ul><li><p>类的创建</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Dog</span>():<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,name,age</span>):<br><span class="hljs-variable language_">self</span>.name = name<br><span class="hljs-variable language_">self</span>.age = age<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sit</span>(<span class="hljs-params">self</span>):<br><span class="hljs-built_in">print</span>(<span class="hljs-variable language_">self</span>.name.title() + <span class="hljs-string">&quot;is now sitting.&quot;</span>)<br></code></pre></td></tr></table></figure><p>**方法__init__()**在每次创建新实例时，Python都会自动运行它。其中形参self必不可少，且位于所有形参的最前面。Python在调用__init__()方法创建实例时，会自动传入实参self，让实例能访问类中的属性与方法。</p></li><li><p>根据类创建实例以及属性访问、方法调用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">my_dog = Dog(<span class="hljs-string">&#x27;willie&#x27;</span>,<span class="hljs-number">6</span>)<br>my_dog.name<br>my_dog.sit()<br></code></pre></td></tr></table></figure></li></ul><h3 id="使用类和实例"><a href="#使用类和实例" class="headerlink" title="使用类和实例"></a>使用类和实例</h3><ul><li><p>通过方法修改属性的值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,model</span>):<br><span class="hljs-variable language_">self</span>.model = model<br><span class="hljs-variable language_">self</span>.miles = <span class="hljs-number">0</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self,mileage</span>)<br><span class="hljs-variable language_">self</span>.miles = mileage<br><br><br><span class="hljs-comment"># 调用</span><br>my_car = Car(<span class="hljs-string">&#x27;audi&#x27;</span>,<span class="hljs-number">2015</span>)<br>my_car.update(<span class="hljs-number">20</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><ul><li><p>子类的方法__init__()</p><p>定义子类时，必须在括号内指定父类的名称。super()函数将父类与子类关联起来，父类称为超类(superclass)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Elecar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,model,year</span>):<br><span class="hljs-comment"># 初始化父类属性</span><br><span class="hljs-built_in">super</span>().__init__(model,year)<br><span class="hljs-variable language_">self</span>.battery_size = <span class="hljs-number">70</span><br><br>my_tesla = Elecar(<span class="hljs-string">&#x27;tesla&#x27;</span>,<span class="hljs-number">2016</span>)<br></code></pre></td></tr></table></figure></li><li><p>重写父类的方法</p><p>可在子类中重写父类包含的方法，调用该方法时将忽略父类中的方法</p></li><li><p>将实例用作属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Car</span>():<br><span class="hljs-keyword">pass</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Battery</span>():<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,battery_size=<span class="hljs-number">70</span></span>):<br><span class="hljs-variable language_">self</span>.battery_size = battery_size<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">describe</span>(<span class="hljs-params">self</span>):<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;This car has a &quot;</span> + <span class="hljs-built_in">str</span>(<span class="hljs-variable language_">self</span>.battery_size) + <span class="hljs-string">&quot;-kWh battery&quot;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Elecar</span>(<span class="hljs-title class_ inherited__">Car</span>):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self,model,year</span>):<br><span class="hljs-built_in">super</span>().__init__(model,year)<br><span class="hljs-variable language_">self</span>.battery = Battery()<br><br><br>my_tesla = Elecar(<span class="hljs-string">&#x27;tesla&#x27;</span>,<span class="hljs-number">2016</span>)<br>my_tesla.battery.describe()<br></code></pre></td></tr></table></figure></li></ul><h2 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h2><h3 id="argparse模块的使用"><a href="#argparse模块的使用" class="headerlink" title="argparse模块的使用"></a>argparse模块的使用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> argparse<br><br><span class="hljs-comment"># 创建解释器</span><br>parser = argparse.ArgmentParser(description=<span class="hljs-string">&quot;当前文件描述&quot;</span>)<br><br><span class="hljs-comment"># 添加optional argument，默认是可选的，即可以不用填，有默认值</span><br>parser.add_argument(<span class="hljs-string">&quot;--a&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">5</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;operator A&quot;</span>)<br><br><span class="hljs-comment"># 添加positional argument，默认是不可选的，即必须要填</span><br>parser.add_argument(<span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;xxx&quot;</span>)<br><br><span class="hljs-comment"># 添加action argument，给定&quot;store_true&quot;后，命令行调用后为True，未调用为False</span><br>parser.add_argument(<span class="hljs-string">&quot;--verbose&quot;</span>, action=<span class="hljs-string">&quot;store_true&quot;</span>, default=<span class="hljs-number">0</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&quot;Print Message&quot;</span>)<br><br><span class="hljs-comment"># 解析命令行</span><br>args = parser.parse_args()<br></code></pre></td></tr></table></figure><p>命令行的操作：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">python test.py --<span class="hljs-built_in">help</span><br>python test.py --a=1 --verbose<br></code></pre></td></tr></table></figure><h1 id="Pytorch基础语法"><a href="#Pytorch基础语法" class="headerlink" title="Pytorch基础语法"></a>Pytorch基础语法</h1><p><strong>Pytorch Documention</strong>:<a href="https://pytorch.org/docs/stable/index.html">PyTorch documentation — PyTorch 2.0 documentation</a></p><h2 id="深度学习基本概念"><a href="#深度学习基本概念" class="headerlink" title="深度学习基本概念"></a>深度学习基本概念</h2><h3 id="epoch与batch"><a href="#epoch与batch" class="headerlink" title="epoch与batch"></a>epoch与batch</h3><ul><li>epoch - 指将整个训练数据集完整过一遍的次数。在每个epoch中，算法将使用训练数据集中的每个样本进行前向传播、计算损失、反向传播和参数更新。训练过程通常会通过多个epoch来不断迭代，以逐渐优化模型的性能。一个epoch的完成意味着模型已经使用了训练数据集中的所有样本进行了一次训练。</li><li>batch - 指将训练数据集划分为小块进行训练的方式。由于训练数据集通常很大，无法一次性全部加载到内存中进行处理，所以将其划分为较小的批次进行训练。每个批次包含多个样本，通常是2的幂次方（如32、64、128等），以便更好地利用硬件加速器（如GPU）的并行计算能力。在每个batch中，模型将针对该批次中的样本进行前向传播、计算损失、反向传播和参数更新。</li></ul><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="torchvision概念"><a href="#torchvision概念" class="headerlink" title="torchvision概念"></a>torchvision概念</h3><p>torchvision用于处理图像数据，主要包含以下四部分：</p><ul><li>torchvision - 提供各种经典网络、预训练好的模型，如Alex-Net、VGG、ResNet、Inception等。</li><li>torchvison.datasets - 提供常用的数据集，设计上继承torch.utils.data.Dataset，主要包括：MNIST、CIFAR10&#x2F;100、ImageNet、COCO等。</li><li>torchvison.transforms - 提供常用的数据预处理操作，主要包括对tensor和PIL Image对象的操作。</li><li>torchvision.utils - 工具类，如保存张量作为图像到磁盘，给一个小批量创建一个图像网格。</li></ul><h2 id="torchvision模型操作"><a href="#torchvision模型操作" class="headerlink" title="torchvision模型操作"></a>torchvision模型操作</h2><h3 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h3><p>将参数pretrained(默认值为<code>False</code>)设为<code>True</code>可以加载预训练模型。</p><p>预训练模型的输入：</p><ul><li>RGB图像的mini-batch:(batch_size,3,H,W)，并且H和W不能低于224。</li><li>像素值必须在范围[0,1]间。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models<br><br>resnet18 = models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br>vgg16 = models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 使用state_dict()来获取状态参数、缓存</span><br>pretrained_dict = vgg16.state_dict()<br></code></pre></td></tr></table></figure><h3 id="现有模型的修改"><a href="#现有模型的修改" class="headerlink" title="现有模型的修改"></a>现有模型的修改</h3><p>对预训练的模型可以进行结构的修改。如ResNet最后全连接层是分1000个类，可以修改为指定的类别数；或ResNet第一层卷积接收的通道是3， 我们可能输入图片的通道是4。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 修改通道数</span><br>resnet.conv1 = nn.Conv2d(<span class="hljs-number">4</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 这里的21即是分类</span><br>resnet.fc = nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">21</span>)<br><br><br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-comment"># 加载预训练好的模型，保存到 ~/.torch/models/ 下面</span><br>resnet34 = models.resnet34(pretrained=<span class="hljs-literal">True</span>, num_classes=<span class="hljs-number">1000</span>)<br><span class="hljs-comment"># 默认是ImageNet上的1000分类，这里修改最后的全连接层为10分类问题</span><br>resnet34.fc = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>)<br></code></pre></td></tr></table></figure><h3 id="网络模型的保存与读取"><a href="#网络模型的保存与读取" class="headerlink" title="网络模型的保存与读取"></a>网络模型的保存与读取</h3><p>模型保存读取有两种方式，即保存模型结构与参数以及以字典形式保存模型参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 保存方式1，模型结构+模型参数</span><br>torch.save(vgg16,<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)<br><br><span class="hljs-comment"># 加载方式1</span><br>model = torch.load(<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)<br><br><span class="hljs-comment"># 保存方式2，模型参数</span><br>torch.save(vgg16.state_dict(),<span class="hljs-string">&quot;vgg16_method2.pth&quot;</span>)<br><br><span class="hljs-comment"># 加载方式2</span><br>model.load_state_dict(torch.model.load(<span class="hljs-string">&quot;vgg16_method2.pth&quot;</span>))<br></code></pre></td></tr></table></figure><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><h3 id="Dataset概念"><a href="#Dataset概念" class="headerlink" title="Dataset概念"></a>Dataset概念</h3><p>Dataset类的作用：提供一种方式去获取数据及其对应的真实标签。该类是一个抽象类，所有的数据集想要在数据与标签之间建立映射，都需要继承这个类，所有的子类都需要重写__getitem__方法，该方法根据索引值获取每一个数据并且获取其对应的标签，子类也可以重写__len__方法，返回数据集的大小</p><p>在Dataset类的子类中，有以下函数以实现部分功能：</p><ul><li>获取每一个数据及其对应的标签，用于模型训练。</li><li>统计数据集中的数据数量，从而能确定迭代次数。</li></ul><h3 id="构建实例"><a href="#构建实例" class="headerlink" title="构建实例"></a>构建实例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GetData</span>(<span class="hljs-title class_ inherited__">Dataset</span>): <span class="hljs-comment"># 继承Dataset类</span><br>    <span class="hljs-comment"># 初始化为整个class提供全局变量，为后续方法提供一些量</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root_dir, label_dir</span>):<br>        <span class="hljs-variable language_">self</span>.root_dir = root_dir<span class="hljs-comment"># 获得根目录路径</span><br>        <span class="hljs-variable language_">self</span>.label_dir = label_dir<span class="hljs-comment"># 获得子目录路径</span><br>        <span class="hljs-variable language_">self</span>.path = os.path.join(<span class="hljs-variable language_">self</span>.root_dir, <span class="hljs-variable language_">self</span>.label_dir)<span class="hljs-comment"># 将路径拼接</span><br>        <span class="hljs-variable language_">self</span>.img_path_list = os.listdir(<span class="hljs-variable language_">self</span>.path)<span class="hljs-comment"># listdir方法会将路径下的所有文件名（包括后缀名）组成一个列表</span><br><br>    <span class="hljs-comment"># 如果在类中定义了__getitem__()方法，那么他的实例对象就可以使用索引方法取值。</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        img_name = <span class="hljs-variable language_">self</span>.img_path_list[idx]  <span class="hljs-comment"># 只获取了文件名</span><br>        img_item_path = os.path.join(<span class="hljs-variable language_">self</span>.root_dir, <span class="hljs-variable language_">self</span>.label_dir, img_name) <span class="hljs-comment"># 获取每个图片的路径位置</span><br>        <span class="hljs-comment"># 读取图片</span><br>        img = Image.<span class="hljs-built_in">open</span>(img_item_path)<br>        label = <span class="hljs-variable language_">self</span>.label_dir<br>        <span class="hljs-keyword">return</span> img, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.img_path)<span class="hljs-comment"># 返回数据数量</span><br><br>root_dir = <span class="hljs-string">&quot;dataset/train&quot;</span><br>ants_label_dir = <span class="hljs-string">&quot;ants_image&quot;</span><br>bees_label_dir = <span class="hljs-string">&quot;bees_image&quot;</span><br>ants_dataset = GetData(root_dir, ants_label_dir)<br>bees_dataset = GetData(root_dir, bees_label_dir)<br>img, lable = ants_dataset[<span class="hljs-number">0</span>] <span class="hljs-comment"># 返回一个元组，返回值就是__getitem__的返回值</span><br><br><br><span class="hljs-comment"># 获取整个训练集，就是对两个数据集进行了拼接</span><br>train_dataset = ants_dataset + bees_dataset<br><br>len1 = <span class="hljs-built_in">len</span>(ants_dataset)  <span class="hljs-comment"># 124</span><br>len2 = <span class="hljs-built_in">len</span>(bees_dataset)  <span class="hljs-comment"># 121</span><br><span class="hljs-built_in">len</span> = <span class="hljs-built_in">len</span>(train_dataset) <span class="hljs-comment"># 245</span><br><br>img1, label1 = train_dataset[<span class="hljs-number">123</span>]  <span class="hljs-comment"># 获取的是蚂蚁的最后一个</span><br>img2, label2 = train_dataset[<span class="hljs-number">124</span>]  <span class="hljs-comment"># 获取的是蜜蜂第一个</span><br></code></pre></td></tr></table></figure><h2 id="transforms"><a href="#transforms" class="headerlink" title="transforms"></a>transforms</h2><h3 id="transforms概念"><a href="#transforms概念" class="headerlink" title="transforms概念"></a>transforms概念</h3><p>transforms主要用于对图像数据进行预处理。</p><h3 id="transforms操作"><a href="#transforms操作" class="headerlink" title="transforms操作"></a>transforms操作</h3><p>transforms的操作可以通过<code>torchvision.transforms.Compose</code>整合在一起进行操作，具体包含的部分操作如下：</p><ul><li><p><code>ToTensor()</code> - 把图片数据转换成张量并使其范围在[0,1]内。</p></li><li><p><code>Normalization(mean,std)</code> - 归一化。</p></li><li><p><code>Resize(size)</code> - 输入的PIL图像调整为指定的大小，参数可以为int或int元组。</p></li><li><p><code>CenterCrop(size)</code> - 将给定的PIL Image进行中心切割，得到指定大小的元组。</p></li><li><p><code>RandomCrop(size,padding=0)</code> - 随机中心点切割。</p></li><li><p><code>RandomHorizontalFlip(size,interpolation=2)</code> - 将给定的PIL Image随机切割，再进行Resize。</p></li><li><p><code>RandomHorizontalFlip()</code> - 随机水平翻转给定的PIL Image。</p></li><li><p><code>RandomVerticalFlip()</code> - 随机垂直翻转给定的PIL Image。</p></li><li><p><code>ToPILImage()</code> - 将Tensor或者numpy.ndarray转换为PIL Image。</p></li><li><p><code>FiveCrop(size)</code> - 将给定的PIL Image裁剪成4个角落区域和中心区域。</p></li><li><p><code>Pad(padding,fill=0,padding_mode=&#39;constant&#39;)</code> - 对PIL边缘进行填充。</p></li><li><p><code>RandomAffline(degrees,translate=None,scale=None)</code> - 保存中心不变的图片进行随机仿射变换。</p></li><li><p><code>RandomApply(transforms,p=0.5)</code> - 随机选取变换。</p></li></ul><h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><h3 id="DataLoader-1"><a href="#DataLoader-1" class="headerlink" title="DataLoader"></a>DataLoader</h3><p>DataLoader是Pytorch中用来处理模型输入数据的一个工具类，组合了数据集(dataset)和采样器(sampler)，并在数据集上提供单线程或多线程的可迭代对象。DataLoader的部分概念如下所示：</p><ul><li><strong>epoch</strong> - 所有训练样本输入到模型中称为一个epoch</li><li><strong>iteration</strong> - 一批样本输入到模型中，称为一个iteration</li><li><strong>batchsize</strong> - 批大小，决定一个epoch有多少个iteration，$\text{iteration}&#x3D;\frac{\text{epoch}}{\text{batchsize}}$</li></ul><p>DataLoader的参数如下：</p><ul><li><p><strong>dataset</strong> (<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Dataset"><em>Dataset</em></a>) – 决定从哪里读取数据集。</p></li><li><p><strong>batch_size</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – 每批训练的样本数(默认值为<code>1</code>)。</p></li><li><p><strong>shuffle</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – 每一个epoch是否为乱序(默认为<code>False</code>)。</p></li><li><p><strong>sampler</strong> (<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler"><em>Sampler</em></a> <em>or</em> <em>Iterable,</em> <em>optional</em>) – <code>sampler</code> 是 PyTorch 中用于控制数据加载顺序的对象，它定义了在数据加载过程中如何对样本进行采样和排序的逻辑。因此选择<code>sampler</code>后<code>shuffle</code>参数不需要指定。常见的<code>sampler</code>的定义如下：</p><ul><li><code>SequentialSampler</code>：顺序采样器，按照数据集中样本的顺序逐个采样，即按照索引依次获取样本。适用于不需要对样本顺序进行改变的情况。</li><li><code>RandomSampler</code>：随机采样器，随机地从数据集中采样样本，可以用于训练集的随机采样和验证集的无重复采样。</li><li><code>SubsetRandomSampler</code>：子集随机采样器，从指定的样本子集中随机采样样本。适用于需要从数据集中选择特定样本子集进行训练的情况。</li><li><code>WeightedRandomSampler</code>：加权随机采样器，根据每个样本的权重进行随机采样。适用于不平衡数据集（imbalanced dataset）中的样本采样，可以提高少数类样本的采样概率。</li><li><code>BatchSampler</code>：批次采样器，将样本索引划分为多个批次，每个批次中的样本索引按照指定的规则进行采样。可以用来实现自定义的样本采样逻辑，如带有样本约束条件的采样。</li></ul></li><li><p><strong>batch_sampler</strong> (<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.Sampler"><em>Sampler</em></a> <em>or</em> <em>Iterable</em><em>,</em> <em>optional</em>) – like <code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code>.</p></li><li><p><strong>num_workers</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional</em>) – 是否采用多线程读取数据(默认为<code>0</code>)。<code>num_workers</code> 参数用于指定 DataLoader 中用于加载数据的子进程的数量。每个子进程都是一个独立的工作单元，负责从存储设备中读取数据、进行预处理并返回给主进程。增加 <code>num_workers</code> 的值可以提高数据加载的速度，特别是在数据加载和预处理过程比较耗时时，可以充分利用多核处理器的计算能力。</p><p>需要注意的是，增加 <code>num_workers</code> 的值并不总是能够线性地提高数据加载的速度，因为子进程的数量过多也会导致进程间的通信和调度开销增加。在选择合适的 <code>num_workers</code> 值时，需要根据具体的硬件环境、数据集大小和数据加载的复杂度进行调优。</p></li><li><p><strong>collate_fn</strong> (<em>Callable,</em> <em>optional</em>) – 定义如何对每个样本的特征和标签进行处理，并将它们组合成一个批次（batch）的数据。</p><p><code>collate_fn</code> 函数会接收一个样本列表作为输入，并返回一个批次的数据作为输出。在 <code>collate_fn</code> 函数中，可以针对不同的数据类型（如图像、文本等）进行自定义的处理和转换操作，以适应模型的输入要求。</p><p>通常，<code>collate_fn</code> 函数的输入是一个样本列表，每个样本是数据集中的一个元素。每个样本可以是一个元组或字典，其中包含了样本的特征和标签等信息。<code>collate_fn</code> 函数需要将样本列表中的特征和标签分别提取出来，并进行适当的处理和转换，最终返回一个包含批次数据的对象（如张量、列表等）。</p></li><li><p><strong>pin_memory</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, the data loader will copy Tensors into device&#x2F;CUDA pinned memory before returning them. If your data elements are a custom type, or your <code>collate_fn</code> returns a batch that is a custom type, see the example below.</p></li><li><p><strong>drop_last</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – 当样本数不能被batchsize整除时，如果为<code>True</code>，舍弃最后一批不完整的数据(默认为<code>False</code>)。</p></li><li><p><strong>timeout</strong> (<em>numeric,</em> <em>optional</em>) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: <code>0</code>)</p></li><li><p><strong>worker_init_fn</strong> (<em>Callable,</em> <em>optional</em>) – If not <code>None</code>, this will be called on each worker subprocess with the worker id (an int in <code>[0, num_workers - 1]</code>) as input, after seeding and before data loading. (default: <code>None</code>)</p></li><li><p><strong>generator</strong> (<a href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><em>torch.Generator</em></a><em>,</em> <em>optional</em>) – If not <code>None</code>, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate base_seed for workers. (default: <code>None</code>)</p></li><li><p><strong>prefetch_factor</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em> <em>optional,</em> <em>keyword-only arg</em>) – Number of batches loaded in advance by each worker. <code>2</code> means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers&#x3D;0 default is <code>None</code>. Otherwise if value of num_workers&gt;0 default is <code>2</code>).</p></li><li><p><strong>persistent_workers</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em> <em>optional</em>) – If <code>True</code>, the data loader will not shutdown the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive. (default: <code>False</code>)</p></li><li><p><strong>pin_memory_device</strong> (<a href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em> <em>optional</em>) – the data loader will copy Tensors into device pinned memory before returning them if pin_memory is set to true.</p></li></ul><h3 id="DataLoader的使用"><a href="#DataLoader的使用" class="headerlink" title="DataLoader的使用"></a>DataLoader的使用</h3><p>以数据集CIFAR10为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./CIFAR10&quot;</span>,<span class="hljs-comment"># 一般将数据集保存在同一工程文件下，使用相对路径读取</span><br>                                         train=<span class="hljs-literal">False</span>,<span class="hljs-comment"># 选择测试集</span><br>                                         transform=torchvision.transforms.ToTensor())<br>test_loader = DataLoader(dataset=test_data, <span class="hljs-comment"># 选择数据集</span><br>                         batch_size=<span class="hljs-number">4</span>, <span class="hljs-comment"># 设定批大小为4</span><br>                         shuffle=<span class="hljs-literal">True</span>, <span class="hljs-comment"># 打乱数据集</span><br>                         num_workers=<span class="hljs-number">0</span>, <span class="hljs-comment"># 单进程读取数据</span><br>                         drop_last=<span class="hljs-literal">False</span>) <span class="hljs-comment"># 保留最后一批数据</span><br><br><span class="hljs-comment"># 测试数据集中第一张图片及target</span><br>img, target = test_data[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(img.shape)<br><span class="hljs-built_in">print</span>(target)<br>------------------------------<br><span class="hljs-comment"># 输出</span><br>torch.Size([<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>])<span class="hljs-comment"># 输出图片为3通道，大小为32*32</span><br><span class="hljs-number">3</span><span class="hljs-comment"># 输出图片标签为3</span><br>------------------------------<br><br><span class="hljs-comment"># 在定义test_loader时，设置了batch_size=4，表示一次性从数据集中取出4个数据</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>    imgs, targets = data<br>    <span class="hljs-built_in">print</span>(imgs.shape)<br>    <span class="hljs-built_in">print</span>(targets)<br>------------------------------<br><span class="hljs-comment"># 第一次循环输出</span><br>torch.Size([<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>])<span class="hljs-comment"># 4表示batch_size=4，后面三个参数表示输出图片为3通道，大小为32*32</span><br>tensor([<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,<span class="hljs-number">3</span>,<span class="hljs-number">8</span>])<span class="hljs-comment"># 表示该批取出的图片的标签信息</span><br>------------------------------<br></code></pre></td></tr></table></figure><h2 id="模型搭建"><a href="#模型搭建" class="headerlink" title="模型搭建"></a>模型搭建</h2><h3 id="nn-Module类中的常用函数"><a href="#nn-Module类中的常用函数" class="headerlink" title="nn.Module类中的常用函数"></a>nn.Module类中的常用函数</h3><h4 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h4><p><code>nn.Conv2d</code>定义了一个卷积层，参数如下：</p><p>计算前后两个卷积层的<code>kernel_size</code>的公式：<br>$$<br>\text{Output}&#x3D;\frac{\text{Width}-\text{kernel_size}+2*\text{Padding}}{\text{Stride}}+1<br>$$<br>默认情况下，$\text{Padding}&#x3D;0,\text{Stride&#x3D;1}$，则有<br>$$<br>\text{Output}&#x3D;\text{Width}-\text{kernel_size}+1<br>$$</p><h3 id="使用nn-Module类搭建模块与模型"><a href="#使用nn-Module类搭建模块与模型" class="headerlink" title="使用nn.Module类搭建模块与模型"></a>使用nn.Module类搭建模块与模型</h3><p>在自定义网络时，需要继承<code>nn.Module</code>类，并重新实现构造<code>__init__</code>构造函数和<code>forward</code>这两个函数。<strong>其中<code>forward</code>函数是必须要重写的，它能实现各个层之间的连接关系</strong>。</p><p>一般的，在<code>__init__</code>中实现层的参数设定，在<code>forward</code>中实现层之间的连接关系。<strong>所有放在<code>__init__</code>里面的层都是这个模型的固有属性</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyNet, <span class="hljs-variable language_">self</span>).__init__()  <span class="hljs-comment"># 调用父类的构造函数</span><br>        <span class="hljs-variable language_">self</span>.conv1 = torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.relu1=torch.nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.max_pooling1=torch.nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br> <br>        <span class="hljs-variable language_">self</span>.conv2 = torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.relu2=torch.nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.max_pooling2=torch.nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br> <br>        <span class="hljs-variable language_">self</span>.dense1 = torch.nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">3</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.dense2 = torch.nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">10</span>)<br> <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = <span class="hljs-variable language_">self</span>.conv1(x)<br>        x = <span class="hljs-variable language_">self</span>.relu1(x)<br>        x = <span class="hljs-variable language_">self</span>.max_pooling1(x)<br>        x = <span class="hljs-variable language_">self</span>.conv2(x)<br>        x = <span class="hljs-variable language_">self</span>.relu2(x)<br>        x = <span class="hljs-variable language_">self</span>.max_pooling2(x)<br>        x = <span class="hljs-variable language_">self</span>.dense1(x)<br>        x = <span class="hljs-variable language_">self</span>.dense2(x)<br>        <span class="hljs-keyword">return</span> x<br> <br>model = MyNet()<br></code></pre></td></tr></table></figure><p>此外，还可以使用<code>Sequential</code>容器实现层之间的连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyNet, <span class="hljs-variable language_">self</span>).__init__()  <span class="hljs-comment"># 调用父类的构造函数</span><br>        <span class="hljs-variable language_">self</span>.conv1 = torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.relu1=torch.nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.max_pooling1=torch.nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br> <br>        <span class="hljs-variable language_">self</span>.conv2 = torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.relu2=torch.nn.ReLU()<br>        <span class="hljs-variable language_">self</span>.max_pooling2=torch.nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br> <br>        <span class="hljs-variable language_">self</span>.dense1 = torch.nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">3</span>, <span class="hljs-number">128</span>)<br>        <span class="hljs-variable language_">self</span>.dense2 = torch.nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">10</span>)<br> <br>        <span class="hljs-variable language_">self</span>.model = nn.Sequential(<br>        nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>),<br>            nn.ReLU(),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>,<span class="hljs-number">1</span>),<br>            nn.Linear(<span class="hljs-number">32</span> * <span class="hljs-number">3</span> * <span class="hljs-number">3</span>, <span class="hljs-number">128</span>),<br>            nn.Linear(<span class="hljs-number">128</span>,<span class="hljs-number">10</span>)<br>        )<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.model(x)<br> <br>model = MyNet()<br></code></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>GPU训练，对模型、数据、损失函数使用CUDA方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 方式1，直接调用CUDA</span><br>model = Model()<br>model = model.cuda()<br><br><span class="hljs-comment"># 方式2，使用to方法选择CPU、CUDA进行训练</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>imgs, labels = data<br>imgs, labels = imgs.to(device), labels.to(device)<br></code></pre></td></tr></table></figure><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><h2 id="自定义层的实现"><a href="#自定义层的实现" class="headerlink" title="自定义层的实现"></a>自定义层的实现</h2><p>要实现一个自定义层，需要有以下步骤：</p><ul><li>自定义一个类，该类继承<code>nn.Module</code>类，并且要实现基本函数：<code>__init__</code>构造函数和<code>forward</code>逻辑运算函数。</li><li>在<code>__init__</code>中实现层的参数定义</li><li>在<code>forward</code>中实现批数据的前向传播，<strong>只要在<code>nn.Module</code>的子类中定义了<code>forward</code>函数，<code>backward</code>函数会自动实现</strong>。但是如果自定义参数不可导，就需要手动实现<code>backward</code>函数。</li></ul><p>自定义层实现这样一个功能：输入为两个$N$维向量$x_1$和$x_2$，参数为一个$N\times N$的矩阵$M$，输出为$\text{label}&#x3D;\text{sigmoid}(x_1\times M\times x_2)$。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable<br><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<br><span class="hljs-keyword">import</span> torch.utils.data <span class="hljs-keyword">as</span> Data<br> <br> <br><span class="hljs-comment"># 定义DisMult层</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DisMult</span>(nn.Module):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, emb_size</span>):<br> <br><span class="hljs-comment"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span><br><span class="hljs-comment"># 下式等价于nn.Module.__init__(self)</span><br><span class="hljs-built_in">super</span>(DisMult, <span class="hljs-variable language_">self</span>).__init__()<br><span class="hljs-comment"># 隐特征维度</span><br><span class="hljs-variable language_">self</span>.emb_size = emb_size<br><span class="hljs-comment"># 关系特定的方阵</span><br><span class="hljs-comment"># self.weights = nn.Parameter(torch.Tensor(emb_size, emb_size), requires_grad=requires_grad)</span><br><span class="hljs-variable language_">self</span>.weights = nn.Parameter(torch.Tensor(emb_size, emb_size))<br><span class="hljs-comment"># 初始化参数</span><br><span class="hljs-variable language_">self</span>.reset_parameters()<br> <br><span class="hljs-comment"># 初始化参数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reset_parameters</span>(<span class="hljs-params">self</span>):<br>stdv = <span class="hljs-number">1.</span> / math.sqrt(<span class="hljs-variable language_">self</span>.weights.size(<span class="hljs-number">0</span>))<br><span class="hljs-variable language_">self</span>.weights.data.uniform_(-stdv, stdv)<br> <br><span class="hljs-comment"># 前向传播函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input1, input2</span>):<br><span class="hljs-comment"># 前向传播的逻辑</span><br>result = torch.<span class="hljs-built_in">sum</span>((input1 @ <span class="hljs-variable language_">self</span>.weights) * input2, dim=<span class="hljs-number">1</span>)<br> <br><span class="hljs-keyword">return</span> torch.sigmoid(result)<br></code></pre></td></tr></table></figure><h2 id="backward的相关内容"><a href="#backward的相关内容" class="headerlink" title="backward的相关内容"></a><code>backward</code>的相关内容</h2>]]></content>
    
    
    <categories>
      
      <category>Pytorch</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
